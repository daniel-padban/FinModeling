{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: {'seq_len': 60, 'pred_len': 12, 'train_start': '2020-01-01', 'train_end': '2022-12-31', 'test_start': '2023-01-01', 'test_end': '2023-12-31', 'hidden': 500, 'n_lstm': 3, 'batch_size': 3, 'lr': 0.0003, 'epochs': 10}\n",
      "Tickers: ['ABB.ST', 'ALFA.ST', 'ASSA-B.ST', 'ATCO-A.ST', 'ATCO-B.ST', 'AZN.ST', 'BOL.ST', 'ELUX-B.ST', 'ERIC-B.ST', 'ESSITY-B.ST', 'EVO.ST', 'GETI-B.ST', 'HEXA-B.ST', 'HM-B.ST', 'INVE-B.ST', 'KINV-B.ST', 'NDA-SE.ST', 'NIBE-B.ST', 'SAAB-B.ST', 'SAND.ST', 'SBB-B.ST', 'SCA-B.ST', 'SEB-A.ST', 'SHB-A.ST', 'SINCH.ST', 'SKF-B.ST', 'SWED-A.ST', 'TEL2-B.ST', 'TELIA.ST', 'VOLV-B.ST']\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "from JSONReader import read_json\n",
    "config_path = 'config.json'\n",
    "config_dict = read_json(config_path)\n",
    "OMXS30_tickers = read_json('omxs30.json')\n",
    "print(f\"Configuration: {config_dict}\")\n",
    "print(f\"Tickers: {OMXS30_tickers}\")\n",
    "\n",
    "import torch\n",
    "device = torch.device(\n",
    "    'cuda' \n",
    "    if torch.cuda.is_available()\n",
    "    else 'mps' \n",
    "    if torch.backends.mps.is_available() \n",
    "    else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataset_def import ReturnDataset\n",
    "import datetime\n",
    "seq_len = config_dict['seq_len']\n",
    "pred_len = config_dict['pred_len']\n",
    "batch_size = config_dict['batch_size']\n",
    "\n",
    "train_start = datetime.datetime.strptime(config_dict['train_start'],'%Y-%m-%d')\n",
    "train_end = datetime.datetime.strptime(config_dict['train_end'],'%Y-%m-%d')\n",
    "\n",
    "train_dataset = ReturnDataset(\n",
    "    seq_len=seq_len,\n",
    "    prediction_len=pred_len,\n",
    "    ticker_list=OMXS30_tickers ,\n",
    "    start_date=train_start,\n",
    "    end_date=train_end)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size,shuffle=False) #data is already shuffled\n",
    "\n",
    "test_start = datetime.datetime.strptime(config_dict['test_start'],'%Y-%m-%d')\n",
    "test_end = datetime.datetime.strptime(config_dict['test_end'],'%Y-%m-%d')\n",
    "\n",
    "test_dataset = ReturnDataset(seq_len=seq_len,\n",
    "                             prediction_len=pred_len,\n",
    "                             ticker_list=OMXS30_tickers,\n",
    "                             start_date=test_start,\n",
    "                             end_date=test_end)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size,shuffle=False) #data is already shuffled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length: 60\n",
      "Prediction length: 12\n",
      "Train start-date: 2020-01-01\n",
      "Train end-date: 2022-12-31\n",
      "Test start-date: 2023-01-01\n",
      "Test end-date: 2023-12-31\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sequence length: {seq_len}\")\n",
    "print(f\"Prediction length: {pred_len}\")\n",
    "print(f\"Train start-date: {config_dict['train_start']}\")\n",
    "print(f\"Train end-date: {config_dict['train_end']}\")\n",
    "print(f\"Test start-date: {config_dict['test_start']}\")\n",
    "print(f\"Test end-date: {config_dict['test_end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate LSTM model for return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMPredictor(\n",
       "  (lstm1): LSTM(6, 500, num_layers=3, batch_first=True)\n",
       "  (activation): SiLU()\n",
       "  (fco): Linear(in_features=500, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lstm_predictor import LSTMPredictor\n",
    "\n",
    "input_size = train_dataset.num_feats\n",
    "hidden_size = config_dict['hidden']\n",
    "output_size = pred_len\n",
    "n_layers = config_dict['n_lstm']\n",
    "\n",
    "model = LSTMPredictor(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size,\n",
    "    n_layers=n_layers,\n",
    "    batch_first=True,\n",
    ")\n",
    "model.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model using LSTMTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Full epoch: 1 ----------\n",
      "---------- Full epoch: 2 ----------\n",
      "---------- Full epoch: 3 ----------\n",
      "---------- Full epoch: 4 ----------\n",
      "---------- Full epoch: 5 ----------\n",
      "---------- Full epoch: 6 ----------\n",
      "---------- Full epoch: 7 ----------\n",
      "---------- Full epoch: 8 ----------\n",
      "---------- Full epoch: 9 ----------\n",
      "---------- Full epoch: 10 ----------\n"
     ]
    }
   ],
   "source": [
    "from lstm_trainer import LSTMTrainer\n",
    "\n",
    "trainer = LSTMTrainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    config_dict=config_dict,\n",
    "    train_scaler_dict=train_dataset.scaler_dict,\n",
    "    test_scaler_dict=test_dataset.scaler_dict,\n",
    "    report_freq=100,\n",
    "    device=device,\n",
    ")\n",
    "train_batch_loss, test_batch_loss = trainer.full_epoch_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize model and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "graph_batch,_,_ = next(iter(train_dataloader))\n",
    "if device == torch.device('mps'):\n",
    "    graph_batch_dtype = torch.float32\n",
    "else:\n",
    "    graph_batch_dtype = torch.float64\n",
    "\n",
    "writer.add_graph(model=model, input_to_model=graph_batch.to(device=device,dtype=graph_batch_dtype))\n",
    "mean_epoch_array = train_batch_loss.mean(1)\n",
    "for i in range(train_batch_loss.shape[0]):\n",
    "    writer.add_scalar('train_mean_epoch_loss',scalar_value=mean_epoch_array[i],global_step=i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRB-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
